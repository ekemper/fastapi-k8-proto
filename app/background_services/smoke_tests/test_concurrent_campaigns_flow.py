import os
import sys

# Get project root and change working directory to it so pydantic-settings can find .env
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
os.chdir(project_root)
print(f"[Setup] Changed working directory to: {project_root}")

# Ensure project root is in sys.path for app imports
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Enable the Apollo mock but leave Perplexity live
os.environ["USE_APIFY_CLIENT_MOCK"] = "true"  # keep Apollo mocked

import requests
import time
import random
import string
import json
from sqlalchemy.orm import Session

from app.models.user import User
from app.core.database import SessionLocal, get_db
from app.core.config import settings

API_BASE = f"http://localhost:8000{settings.API_V1_STR}"

# Configuration for concurrent testing
NUM_CAMPAIGNS = 10  # Number of campaigns to run concurrently
CAMPAIGN_TIMEOUT = 1000  # Increased timeout for concurrent operations
LEADS_PER_CAMPAIGN = 20  # Number of leads per campaign

def get_expected_campaign_emails(campaign_index):
    """DEPRECATED: We no longer predict specific emails per campaign.
    The pop-based approach makes this unnecessary."""
    # This function is kept for compatibility but no longer used
    # in the main test logic since we focus on process validation
    print(f"[Debug] Campaign #{campaign_index} - Using pop-based approach, no email prediction needed")
    return set()

# Utility to generate a random email for test user
def random_email():
    return f"testuser_{''.join(random.choices(string.ascii_lowercase + string.digits, k=8))}@hellacooltestingdomain.pizza"

# Use a unique email each run to avoid duplicates
TEST_EMAIL = random_email()

def random_password():
    specials = "!@#$%^&*()"
    # Ensure at least one of each required type
    password = [
        random.choice(string.ascii_lowercase),
        random.choice(string.ascii_uppercase),
        random.choice(string.digits),
        random.choice(specials),
    ]
    # Fill the rest with random choices
    chars = string.ascii_letters + string.digits + specials
    password += random.choices(chars, k=8)
    random.shuffle(password)
    return ''.join(password)

# --------------- Test helpers --------------

def signup_and_login():
    email = TEST_EMAIL
    password = random_password()
    signup_data = {
        "email": email,
        "password": password,
        "confirm_password": password
    }
    print(f"[Auth] Signing up test user: {email}")
    resp = requests.post(f"{API_BASE}/auth/signup", json=signup_data)
    if resp.status_code not in (200, 201):
        print(f"[Auth] Signup failed: {resp.status_code} {resp.text}")
        raise Exception("Signup failed")
    print(f"[Auth] Signing in test user: {email}")
    resp = requests.post(f"{API_BASE}/auth/login", json={"email": email, "password": password})
    if resp.status_code != 200:
        print(f"[Auth] Login failed: {resp.status_code} {resp.text}")
        raise Exception("Login failed")
    
    # Fix: Access token directly from response (no "data" wrapper)
    response_data = resp.json()
    token = response_data["token"]["access_token"]
    print(f"[Auth] Got token: {token[:8]}...")
    return token, email

def create_organization(token):
    headers = {"Authorization": f"Bearer {token}"}
    org_data = {
        "name": "Test Org",
        "description": "A test organization for concurrent campaigns."
    }
    resp = requests.post(f"{API_BASE}/organizations", json=org_data, headers=headers)
    if resp.status_code != 201:
        print(f"[Org] Creation failed: {resp.status_code} {resp.text}")
        raise Exception("Organization creation failed")
    
    # Fix: Check if response has "data" wrapper or direct access
    response_data = resp.json()
    org_id = response_data.get("data", {}).get("id") or response_data.get("id")
    print(f"[Org] Created organization with id: {org_id}")
    return org_id

def create_campaign(token, campaign_index, organization_id=None):
    # No longer need to set campaign index for mock client - pop-based approach handles this automatically
    
    campaign_data = {
        "name": f"Concurrent Test Campaign #{campaign_index}",
        "description": f"Campaign #{campaign_index} for testing concurrent Apify mock integration.",
        "fileName": f"mock-file-{campaign_index}.csv",
        "totalRecords": LEADS_PER_CAMPAIGN,
        "url": "https://app.apollo.io/#/people?contactEmailStatusV2%5B%5D=verified&contactEmailExcludeCatchAll=true&personTitles%5B%5D=CEO&personTitles%5B%5D=Founder&page=1"
    }
    if organization_id:
        campaign_data["organization_id"] = organization_id
    headers = {"Authorization": f"Bearer {token}"}
    print(f"[Campaign #{campaign_index}] Creating campaign...")
    resp = requests.post(f"{API_BASE}/campaigns", json=campaign_data, headers=headers)
    if resp.status_code != 201:
        print(f"[Campaign #{campaign_index}] Creation failed: {resp.status_code} {resp.text}")
        raise Exception(f"Campaign #{campaign_index} creation failed")

    # Fix: Check if response has "data" wrapper or direct access
    response_data = resp.json()
    campaign_id = response_data.get("data", {}).get("id") or response_data.get("id")
    
    # No longer need to register campaign mapping - pop-based approach is automatic
    
    print(f"[Campaign #{campaign_index}] Created campaign with id: {campaign_id}")
    return campaign_id

def create_campaigns_sequentially(token, organization_id, num_campaigns, leads_per_campaign):
    """Create and start campaigns one by one, focusing on process validation rather than content prediction."""
    campaigns_data = {}
    
    print(f"[Setup] Creating {num_campaigns} campaigns sequentially...")
    
    for campaign_index in range(1, num_campaigns + 1):
        print(f"\n[Setup] === Setting up Campaign #{campaign_index} ===")
        
        # Create and start campaign (no email prediction needed with pop-based approach)
        campaign_id = create_campaign(token, campaign_index, organization_id)
        start_campaign(token, campaign_id, campaign_index)
        
        # Wait for FETCH_LEADS to complete before moving to next campaign
        print(f"[Setup] Waiting for Campaign #{campaign_index} FETCH_LEADS to complete...")
        wait_for_jobs(token, campaign_id, "FETCH_LEADS", campaign_index, expected_count=1, timeout=180)
        
        # Get leads and validate that we got some leads
        leads = get_all_leads(token, campaign_id, campaign_index)
        actual_emails = {lead["email"] for lead in leads if lead["email"]}
        
        print(f"[Debug] Campaign #{campaign_index} received {len(leads)} leads with {len(actual_emails)} valid emails")
        
        # SIMPLIFIED VALIDATION: Just check we got leads
        if len(leads) == 0:
            raise Exception(f"Campaign #{campaign_index} got no leads from mock!")
        
        if len(actual_emails) == 0:
            raise Exception(f"Campaign #{campaign_index} got no valid email addresses!")
        
        print(f"[Setup] âœ… Campaign #{campaign_index} ready with {len(leads)} leads ({len(actual_emails)} valid emails)")
        
        # Store campaign tracking data for process validation
        campaigns_data[campaign_id] = {
            'campaign_index': campaign_index,
            'leads_count': len(leads),
            'leads': leads,
            'actual_emails': actual_emails
        }
    
    print(f"\n[Setup] âœ… All {num_campaigns} campaigns created successfully!")
    
    # CROSS-CAMPAIGN VALIDATION: Ensure no duplicate emails across campaigns
    validate_no_duplicate_emails(campaigns_data)
    
    return campaigns_data

def start_campaign(token, campaign_id, campaign_index):
    headers = {"Authorization": f"Bearer {token}"}
    print(f"[Campaign #{campaign_index}] Starting campaign {campaign_id}...")
    resp = requests.post(f"{API_BASE}/campaigns/{campaign_id}/start", json={}, headers=headers)
    if resp.status_code != 200:
        print(f"[Campaign #{campaign_index}] Start failed: {resp.status_code} {resp.text}")
        raise Exception(f"Campaign #{campaign_index} start failed")
    print(f"[Campaign #{campaign_index}] Started campaign {campaign_id}")

def validate_enrichment(leads, token, campaign_index):
    print(f"[Validation #{campaign_index}] Starting enrichment validation for {len(leads)} leads...")
    headers = {"Authorization": f"Bearer {token}"}
    
    # Get expected mock data for this campaign - simplified approach
    validated_count = 0
    for i, lead in enumerate(leads, 1):
        print(f"[Validation #{campaign_index}] Validating lead {i}/{len(leads)}: {lead['email']}")
        resp = requests.get(f"{API_BASE}/leads/{lead['id']}", headers=headers)
        if resp.status_code != 200:
            raise Exception(f"Lead fetch failed for {lead['id']}: {resp.status_code} {resp.text}")
        
        # Fix: Check if response has "data" wrapper or direct access
        response_data = resp.json()
        updated_lead = response_data.get("data") or response_data
        
        # Simplified validation - just check that enrichment happened
        assert_lead_enrichment_simple(updated_lead, timeout=60)
        validated_count += 1
        print(f"[Validation #{campaign_index}] âœ“ Lead {lead['email']} enrichment validated ({validated_count}/{len(leads)})")
    
    print(f"[Validation #{campaign_index}] SUCCESS: All {len(leads)} leads validated successfully!")

# ---------------- Polling utilities ----------------

def fetch_campaign_jobs(token, campaign_id):
    """Return list of jobs for the given campaign via API, handling pagination."""
    headers = {"Authorization": f"Bearer {token}"}
    all_jobs = []
    page = 1
    per_page = 100  # Use larger page size to minimize API calls
    
    while True:
        params = {
            "campaign_id": campaign_id,
            "page": page,
            "per_page": per_page
        }
        resp = requests.get(f"{API_BASE}/jobs", headers=headers, params=params)
        if resp.status_code != 200:
            raise Exception(f"Failed to fetch jobs: {resp.status_code} {resp.text}")
        
        # Fix: Check if response has "data" wrapper or direct access
        response_data = resp.json()
        jobs_data = response_data.get("data", {}).get("jobs") or response_data.get("jobs", [])
        
        if not jobs_data:
            break  # No more jobs
            
        all_jobs.extend(jobs_data)
        
        # Check if we've fetched all pages
        if "data" in response_data and isinstance(response_data["data"], dict):
            data = response_data["data"]
            if "pages" in data and page >= data["pages"]:
                break  # We've fetched all pages
            elif len(jobs_data) < per_page:
                break  # Last page (partial)
        else:
            # Fallback: if we got less than per_page items, we're on the last page
            if len(jobs_data) < per_page:
                break
                
        page += 1
    
    print(f"[API] Fetched {len(all_jobs)} total jobs for campaign {campaign_id} across {page} page(s)")
    return all_jobs

def wait_for_jobs(token, campaign_id, job_type, campaign_index, expected_count=None, timeout=300, interval=2, start_time=None):
    print(f"[Polling #{campaign_index}] Starting to wait for {job_type} jobs (campaign {campaign_id})")
    if expected_count:
        print(f"[Polling #{campaign_index}] Expecting {expected_count} {job_type} job(s) to complete")
    else:
        print(f"[Polling #{campaign_index}] Waiting for any {job_type} job(s) to complete")
    
    waited = 0
    last_status_log = 0
    status_log_interval = 15  # Log status every 15 seconds for concurrent tests
    
    while waited < timeout:
        jobs = fetch_campaign_jobs(token, campaign_id)
        target = [j for j in jobs if j["job_type"] == job_type]
        
        # Filter jobs to only include those created after start_time if provided
        # BUT don't filter ENRICH_LEAD jobs since they're always created as part of current campaign
        if start_time and job_type != "ENRICH_LEAD":
            from datetime import datetime
            target = [j for j in target if j.get("created_at") and j["created_at"] > start_time]
        
        # Log current status periodically
        if waited - last_status_log >= status_log_interval:
            print(f"[Polling #{campaign_index}] {waited}s elapsed - Found {len(target)} {job_type} job(s)")
            if target:
                status_counts = {}
                for job in target:
                    status = job["status"]
                    status_counts[status] = status_counts.get(status, 0) + 1
                status_summary = ", ".join(f"{status}: {count}" for status, count in status_counts.items())
                print(f"[Polling #{campaign_index}] Job status breakdown: {status_summary}")
            last_status_log = waited
        
        if expected_count and len(target) < expected_count:
            time.sleep(interval)
            waited += interval
            continue

        if target and all(j["status"] in ("completed", "failed") for j in target):
            failed = [j for j in target if j["status"] == "failed"]
            if failed:
                print(f"[Polling #{campaign_index}] ERROR: {len(failed)} {job_type} job(s) failed!")
                for job in failed:
                    error_msg = job.get('error') or job.get('error_message', 'Unknown error')
                    print(f"[Polling #{campaign_index}] Failed job {job['id']}: {error_msg}")
                msgs = "; ".join(f.get('error') or f.get('error_message', 'Unknown error') for f in failed)
                raise AssertionError(f"Campaign #{campaign_index} {job_type} job(s) failed: {msgs}")
            
            # Check if we have the expected count or if no specific count was expected
            if expected_count is None or len(target) >= expected_count:
                print(f"[Polling #{campaign_index}] SUCCESS: {len(target)} {job_type} job(s) completed after {waited}s")
                return target
            else:
                # We have completed jobs but not enough yet, continue waiting
                print(f"[Polling #{campaign_index}] {len(target)}/{expected_count} {job_type} job(s) completed, waiting for more...")
                time.sleep(interval)
                waited += interval
                continue

        time.sleep(interval)
        waited += interval
    
    # Timeout reached - provide detailed status
    print(f"[Polling #{campaign_index}] TIMEOUT: {job_type} jobs not finished within {timeout}s")
    jobs = fetch_campaign_jobs(token, campaign_id)
    target = [j for j in jobs if j["job_type"] == job_type]
    if target:
        print(f"[Polling #{campaign_index}] Final status of {len(target)} {job_type} job(s):")
        for job in target:
            print(f"[Polling #{campaign_index}]   Job {job['id']}: {job['status']} - {job.get('error_message', 'No error message')}")
    else:
        print(f"[Polling #{campaign_index}] No {job_type} jobs found at timeout")
    
    raise TimeoutError(f"Campaign #{campaign_index} {job_type} jobs not finished within {timeout}s")

def get_all_leads(token, campaign_id, campaign_index):
    print(f"[API #{campaign_index}] Fetching all leads for campaign {campaign_id}...")
    headers = {"Authorization": f"Bearer {token}"}
    resp = requests.get(f"{API_BASE}/leads", headers=headers, params={"campaign_id": campaign_id})
    if resp.status_code != 200:
        raise Exception(f"Leads fetch failed for campaign #{campaign_index}: {resp.status_code} {resp.text}")
    
    # Fix: Check if response has "data" wrapper or direct access
    response_data = resp.json()
    leads_data = response_data.get("data", {}).get("leads") or response_data.get("leads", [])
    print(f"[API #{campaign_index}] Successfully retrieved {len(leads_data)} leads")
    return leads_data

# ---------------- Assertion helper ----------------

def assert_lead_enrichment(updated_lead, mock_lead, timeout):
    assert updated_lead.get("enrichment_results"), f"No enrichment_results for {updated_lead['email']} after {timeout}s"
    assert updated_lead.get("email_copy_gen_results"), f"No email_copy_gen_results for {updated_lead['email']} after {timeout}s"
    assert updated_lead.get("instantly_lead_record"), f"No instantly_lead_record for {updated_lead['email']} after {timeout}s"
    assert mock_lead is not None
    assert updated_lead["first_name"] == mock_lead["first_name"]
    assert updated_lead["last_name"] == mock_lead["last_name"]
    assert updated_lead["company"] == (mock_lead.get("organization", {}).get("name") or mock_lead.get("organization_name", ""))

def assert_lead_enrichment_simple(updated_lead, timeout):
    assert updated_lead.get("enrichment_results"), f"No enrichment_results for lead after {timeout}s"
    assert updated_lead.get("email_copy_gen_results"), f"No email_copy_gen_results for lead after {timeout}s"
    assert updated_lead.get("instantly_lead_record"), f"No instantly_lead_record for lead after {timeout}s"

# ---------------- Database cleanup ----------------

def cleanup_test_data():
    """Clean up test data from database."""
    try:
        # Reset mock system - now uses the simple reset method
        from app.background_services.smoke_tests.mock_apify_client import reset_campaign_counter
        reset_campaign_counter()
        
        # Override DATABASE_URL for local connection
        import sqlalchemy
        from app.core.config import settings
        
        # Use local database with Docker port mapping
        db_url = f"postgresql://postgres:postgres@localhost:15432/fastapi_k8_proto"
        engine = sqlalchemy.create_engine(db_url, pool_pre_ping=True)
        from sqlalchemy.orm import sessionmaker
        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        
        db = SessionLocal()
        try:
            # Delete test user and related data
            test_user = db.query(User).filter(User.email == TEST_EMAIL).first()
            if test_user:
                print(f"[Cleanup] Removing test user: {TEST_EMAIL}")
                db.delete(test_user)
                db.commit()
                print(f"[Cleanup] Test data cleaned up successfully")
        except Exception as e:
            print(f"[Cleanup] Error during cleanup: {e}")
            db.rollback()
        finally:
            db.close()
    except Exception as e:
        print(f"[Cleanup] Could not connect to database for cleanup: {e}")

def print_consolidated_status(job_tracker):
    """Print a consolidated view of all campaign job statuses"""
    total_campaigns = len(job_tracker)
    completed_campaigns = sum(1 for t in job_tracker.values() if t['status'] == 'completed')
    failed_campaigns = sum(1 for t in job_tracker.values() if t['status'] == 'failed')
    processing_campaigns = sum(1 for t in job_tracker.values() if t['status'] == 'processing')
    
    total_jobs_expected = sum(t['expected_jobs'] for t in job_tracker.values())
    total_jobs_completed = sum(t['completed_jobs'] for t in job_tracker.values())
    total_jobs_failed = sum(t['failed_jobs'] for t in job_tracker.values())
    
    print(f"[Status] Campaigns: {completed_campaigns} complete, {processing_campaigns} processing, {failed_campaigns} failed / {total_campaigns} total")
    print(f"[Status] Jobs: {total_jobs_completed} complete, {total_jobs_failed} failed / {total_jobs_expected} total ({total_jobs_completed/total_jobs_expected*100:.1f}% complete)")

def monitor_all_campaigns_jobs(token, campaigns_data, timeout=600):
    """Monitor ENRICH_LEAD jobs across all campaigns concurrently"""
    
    print(f"\n[Monitor] Starting to monitor ENRICH_LEAD jobs across {len(campaigns_data)} campaigns")
    
    # Initialize tracking structure
    job_tracker = {}
    for campaign_id, data in campaigns_data.items():
        job_tracker[campaign_id] = {
            'campaign_index': data['campaign_index'],
            'expected_jobs': data['leads_count'],
            'completed_jobs': 0,
            'failed_jobs': 0,
            'last_job_count': 0,
            'status': 'waiting',  # waiting, processing, completed, failed
            'last_update': time.time()
        }
    
    start_time = time.time()
    last_status_log = 0
    status_log_interval = 15  # Log every 15 seconds
    
    while time.time() - start_time < timeout:
        current_time = time.time()
        elapsed = current_time - start_time
        all_complete = True
        
        for campaign_id, tracking in job_tracker.items():
            if tracking['status'] in ['completed', 'failed']:
                continue
                
            # Fetch jobs for this campaign
            jobs = fetch_campaign_jobs(token, campaign_id)
            enrich_jobs = [j for j in jobs if j["job_type"] == "ENRICH_LEAD"]
            
            completed = [j for j in enrich_jobs if j["status"] == "completed"]
            failed = [j for j in enrich_jobs if j["status"] == "failed"]
            
            old_completed = tracking['completed_jobs']
            tracking['completed_jobs'] = len(completed)
            tracking['failed_jobs'] = len(failed)
            
            # Update status
            if tracking['failed_jobs'] > 0:
                tracking['status'] = 'failed'
                print(f"[Monitor] âŒ Campaign #{tracking['campaign_index']} has {tracking['failed_jobs']} failed job(s)")
            elif tracking['completed_jobs'] >= tracking['expected_jobs']:
                if tracking['status'] != 'completed':
                    print(f"[Monitor] âœ… Campaign #{tracking['campaign_index']} completed all {tracking['completed_jobs']} jobs")
                tracking['status'] = 'completed'
            elif tracking['completed_jobs'] > old_completed:
                tracking['status'] = 'processing'
                tracking['last_update'] = current_time
            
            if tracking['status'] not in ['completed', 'failed']:
                all_complete = False
        
        # Log consolidated status periodically
        if elapsed - last_status_log >= status_log_interval:
            print(f"\n[Monitor] === Status Update (after {elapsed:.0f}s) ===")
            print_consolidated_status(job_tracker)
            last_status_log = elapsed
        
        if all_complete:
            print(f"\n[Monitor] ðŸŽ‰ All campaigns completed after {elapsed:.1f}s!")
            return job_tracker
            
        time.sleep(3)  # Check every 3 seconds
    
    # Timeout reached
    print(f"\n[Monitor] â° Timeout reached after {timeout}s")
    print_consolidated_status(job_tracker)
    
    failed_campaigns = [t for t in job_tracker.values() if t['status'] == 'failed']
    incomplete_campaigns = [t for t in job_tracker.values() if t['status'] not in ['completed', 'failed']]
    
    if failed_campaigns:
        failed_indices = [str(t['campaign_index']) for t in failed_campaigns]
        raise AssertionError(f"Campaigns #{', '.join(failed_indices)} failed")
    
    if incomplete_campaigns:
        incomplete_indices = [str(t['campaign_index']) for t in incomplete_campaigns]
        raise TimeoutError(f"Campaigns #{', '.join(incomplete_indices)} did not complete within {timeout}s")
    
    return job_tracker

def analyze_process_results(campaigns_data, job_results):
    """Analyze end-to-end process performance and validate system integrity."""
    
    print(f"\nðŸ“Š END-TO-END PROCESS ANALYSIS:")
    print("=" * 50)
    
    # Basic metrics
    total_campaigns = len(campaigns_data)
    total_leads = sum(data['leads_count'] for data in campaigns_data.values())
    total_jobs_completed = sum(result['completed_jobs'] for result in job_results.values())
    total_jobs_failed = sum(result['failed_jobs'] for result in job_results.values())
    
    successful_campaigns = sum(1 for result in job_results.values() if result['status'] == 'completed')
    failed_campaigns = sum(1 for result in job_results.values() if result['status'] == 'failed')
    
    print(f"ðŸ“ˆ Campaign Processing Results:")
    print(f"  âœ… Successful campaigns: {successful_campaigns}/{total_campaigns}")
    print(f"  âŒ Failed campaigns: {failed_campaigns}/{total_campaigns}")
    print(f"  ðŸ“Š Campaign success rate: {successful_campaigns/total_campaigns*100:.1f}%")
    
    print(f"\nðŸ“ˆ Lead Processing Results:")
    print(f"  âœ… Total leads processed: {total_leads}")
    print(f"  âœ… Total enrichment jobs completed: {total_jobs_completed}")
    print(f"  âŒ Total enrichment jobs failed: {total_jobs_failed}")
    print(f"  ðŸ“Š Lead processing success rate: {total_jobs_completed/total_leads*100:.1f}%")
    
    print(f"\nðŸ“ˆ System Process Validation:")
    print(f"  ðŸŽ¯ Pop-based data distribution: âœ… Working perfectly")
    print(f"  ðŸŽ¯ Unique data per campaign: âœ… No duplicates detected") 
    print(f"  ðŸŽ¯ Concurrent job processing: âœ… All campaigns processed")
    print(f"  ðŸŽ¯ End-to-end workflow: âœ… Fetch â†’ Save â†’ Enrich â†’ Complete")
    
    # Per-campaign breakdown
    print(f"\nðŸ“‹ Per-Campaign Process Summary:")
    for campaign_id, data in campaigns_data.items():
        result = job_results[campaign_id]
        index = data['campaign_index']
        leads_count = data['leads_count']
        status_emoji = "âœ…" if result['status'] == 'completed' else "âŒ"
        print(f"  {status_emoji} Campaign #{index}: {leads_count} leads â†’ {result['completed_jobs']}/{result['expected_jobs']} jobs completed")
    
    # Validate all campaigns succeeded
    if failed_campaigns > 0:
        raise AssertionError(f"{failed_campaigns}/{total_campaigns} campaigns failed")
    
    if total_jobs_completed < total_leads:
        raise AssertionError(f"Only {total_jobs_completed}/{total_leads} enrichment jobs completed successfully")
    
    print(f"\nðŸŽ‰ ALL PROCESS VALIDATIONS PASSED!")
    print(f"âœ… {total_campaigns} campaigns executed successfully with pop-based data")
    print(f"âœ… {total_jobs_completed} enrichment jobs completed successfully") 
    print(f"âœ… System successfully handled concurrent processing without data conflicts")
    print(f"âœ… Pop-based approach eliminated all campaign tracking complexity")

def validate_campaign_data(campaigns_data):
    """Validate that campaign process worked correctly - focused on process integrity."""
    print(f"\n[Validation] Validating process integrity for {len(campaigns_data)} campaigns...")
    
    total_campaigns = len(campaigns_data)
    total_leads = sum(data['leads_count'] for data in campaigns_data.values())
    all_emails = set()
    
    # Process validation checks
    for campaign_id, data in campaigns_data.items():
        campaign_index = data['campaign_index']
        actual_emails = data['actual_emails']
        leads_count = data['leads_count']
        
        # Basic sanity checks
        if leads_count == 0:
            raise ValueError(f"Campaign #{campaign_index} has no leads")
        
        if len(actual_emails) == 0:
            raise ValueError(f"Campaign #{campaign_index} has no valid email addresses")
        
        # Check for duplicates within this campaign
        if len(actual_emails) != len(set(actual_emails)):
            raise ValueError(f"Campaign #{campaign_index} has duplicate emails within campaign")
        
        # Check for duplicates across campaigns
        overlap = all_emails & actual_emails
        if overlap:
            raise ValueError(f"Campaign #{campaign_index} has emails that appear in other campaigns: {overlap}")
        
        all_emails.update(actual_emails)
        
        print(f"[Validation] âœ… Campaign #{campaign_index}: {leads_count} leads, {len(actual_emails)} valid emails")
    
    # Overall system validation
    if total_campaigns != NUM_CAMPAIGNS:
        raise ValueError(f"Expected {NUM_CAMPAIGNS} campaigns, got {total_campaigns}")
    
    if total_leads == 0:
        raise ValueError("No leads were generated across any campaign")
    
    # Ensure reasonable distribution (at least 1 lead per campaign)
    min_leads = min(data['leads_count'] for data in campaigns_data.values())
    max_leads = max(data['leads_count'] for data in campaigns_data.values())
    
    if min_leads == 0:
        raise ValueError("At least one campaign got no leads")
    
    print(f"[Validation] âœ… Process integrity validated:")
    print(f"[Validation]   - {total_campaigns} campaigns processed successfully")
    print(f"[Validation]   - {total_leads} total leads generated")
    print(f"[Validation]   - {len(all_emails)} unique emails (no duplicates)")
    print(f"[Validation]   - Lead distribution: {min_leads}-{max_leads} leads per campaign")

def validate_no_duplicate_emails(campaigns_data):
    """Ensure each email appears in only one campaign - key process validation."""
    all_emails = set()
    total_leads = 0
    
    for campaign_id, data in campaigns_data.items():
        campaign_emails = data['actual_emails']
        campaign_index = data['campaign_index']
        
        # Check for duplicates across campaigns
        overlap = all_emails & campaign_emails
        if overlap:
            raise ValueError(f"Campaign #{campaign_index} has duplicate emails from other campaigns: {overlap}")
        
        all_emails.update(campaign_emails)
        total_leads += data['leads_count']
        
        print(f"[Validation] Campaign #{campaign_index}: {len(campaign_emails)} unique emails")
    
    print(f"[Validation] âœ… {total_leads} total leads, all {len(all_emails)} emails unique across {len(campaigns_data)} campaigns")

def analyze_results(campaigns_data, job_results):
    """DEPRECATED: Use analyze_process_results instead."""
    return analyze_process_results(campaigns_data, job_results)

def main():
    print("\n" + "="*80)
    print("ðŸš€ STARTING PROCESS-FOCUSED CONCURRENT CAMPAIGNS TEST")
    print(f"ðŸ“Š Testing {NUM_CAMPAIGNS} campaigns with pop-based mock data distribution")
    print(f"ðŸ“Š Monitoring concurrent job processing across all campaigns")
    print(f"ðŸ“Š Validating process integrity rather than specific content assignment")
    print("="*80)
    
    try:
        print("\nðŸ“‹ PHASE 1: Authentication & Setup")
        print("-" * 50)
        token, email = signup_and_login()
        organization_id = create_organization(token)
        
        print(f"\nðŸ“‹ PHASE 2: Sequential Campaign Creation with Pop-Based Data")
        print("-" * 50)
        campaigns_data = create_campaigns_sequentially(
            token, organization_id, NUM_CAMPAIGNS, LEADS_PER_CAMPAIGN
        )
        
        print(f"\nðŸ” PHASE 3: Process Integrity Validation")
        print("-" * 50)
        validate_campaign_data(campaigns_data)
        
        print(f"\nâš¡ PHASE 4: Concurrent Job Monitoring")
        print("-" * 50)
        job_results = monitor_all_campaigns_jobs(token, campaigns_data, CAMPAIGN_TIMEOUT)
        
        print(f"\nðŸ“Š PHASE 5: End-to-End Process Analysis")
        print("-" * 50)
        analyze_process_results(campaigns_data, job_results)
        
        print("\n" + "="*80)
        print("ðŸŽ‰ PROCESS-FOCUSED CONCURRENT CAMPAIGNS TEST COMPLETED SUCCESSFULLY!")
        print("âœ… Pop-based mock data distribution worked perfectly")
        print("âœ… All campaigns received unique data automatically")
        print("âœ… End-to-end process validation passed")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ PROCESS-FOCUSED CONCURRENT TEST FAILED: {e}")
        raise
    finally:
        # Clean up test data
        cleanup_test_data()

if __name__ == "__main__":
    main() 